{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an OCD MADE and evaluate its performance\n",
    "\n",
    "In this notebook, we will train an Ordered Causal Discovery (OCD) model based on the MADE architecture and evaluate its performance on the set dataset from BNlearn.\n",
    "\n",
    "We use pytorch-lightning to train and evaluate our models. The code for the MADE architecture is based on the code from the `torchde` library but modified to support changing the order of the variables with permutation matrices.\n",
    "\n",
    "## Setup Data\n",
    "### Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to path (to use ocd package)\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# set PYTORCH_ENABLE_MPS_FALLBACK=1 in current environment to enable MPS fallback\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# import packages\n",
    "import dycode  # for dynamic code execution\n",
    "import numpy as np\n",
    "import torch\n",
    "import lightning\n",
    "import lightning.pytorch.callbacks\n",
    "from ocd.training import OrderedTrainingModule\n",
    "from ocd.data import CausalDataModule  # for loading data\n",
    "from ocd.training.terms import OrderedLikelihoodTerm, PermanentMatrixPenalizer\n",
    "from ocd.models.utils import log_prob\n",
    "from ocd.evaluation import shd\n",
    "from ocd.post_processing.pruning import prune, PruningMethod\n",
    "\n",
    "\n",
    "# register torch in dynamic code context\n",
    "dycode.register_context(torch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data\n",
    "You can mention the name of your desired dataset from `bnlearn` in the `name` variable. If you wish to use a dataset that is not available by stock `bnlearn`, you can just mention the download link of the `.bif` file as the `name` variable. This will download the dataset and use it to create the observational and interventional samples for you.\n",
    "\n",
    "As a side note, intervention size is the number of samples per value of the intervened variable. For example if a node takes 3 values and the intervention size is 100, then the number of samples for the intervened node will be 300.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data\n",
    "dm = CausalDataModule(\n",
    "    name=\"asia\",  # change this to your desired dataset from bnlearn\n",
    "    observation_size=2048,  # number of observation samples\n",
    "    intervention_size=256,  # set to 0 for no intervention\n",
    "    batch_size=128,  # batch size for training (and validation if val_size > 0)\n",
    "    num_workers=0,  # set to 0 for no multiprocessing\n",
    "    val_size=0,  # set to 0 for no validation, fraction for relative validation size, or int for absolute validation size\n",
    "    pin_memory=True,  # set to True for faster data transfer to GPU (if available)\n",
    ")\n",
    "\n",
    "# read general stats of the data to setup the model later\n",
    "dm.setup(\"fit\")  # prepare the data to read general stats (e.g. number of features and nodes)\n",
    "in_features = dm.train_data[0].features_values  # number of features (used for setting the input size of the model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model and training parameters\n",
    "tm = OrderedTrainingModule(\n",
    "    # input parameters\n",
    "    in_covariate_features=in_features,  # number of nodes and features in the data\n",
    "    # network architecture\n",
    "    hidden_features_per_covariate=[\n",
    "        [32 for i in range(len(in_features))],\n",
    "        [16 for i in range(len(in_features))],\n",
    "        [8 for i in range(len(in_features))],\n",
    "    ],\n",
    "    batch_norm=False,\n",
    "    # training objective\n",
    "    criterion_args=dict(\n",
    "        terms=[\n",
    "            \"ocd.training.terms.OrderedLikelihoodTerm\",\n",
    "            # PermanentMatrixPenalizer(factor=1), # uncomment this line to add a permanent matrix penalizer\n",
    "        ],\n",
    "    ),\n",
    "    # training parameters\n",
    "    optimizer=[\"torch.optim.AdamW\", \"torch.optim.SGD\"],\n",
    "    optimizer_parameters=[\"model.made\", \"model._gamma\"],\n",
    "    optimizer_is_active=[\n",
    "        \"lambda training_module: True\",\n",
    "        \"lambda training_module: True\",\n",
    "    ],\n",
    "    tau_scheduler=\"lambda training_module: max(0.0005, 0.01 * 0.5 ** (training_module.current_epoch // 5))\",\n",
    "    n_sinkhorn_scheduler=\"lambda training_module: 20\",\n",
    "    lr=[0.001, 1],\n",
    "    scheduler=\"torch.optim.lr_scheduler.ExponentialLR\",\n",
    "    scheduler_interval=\"epoch\",\n",
    "    scheduler_args={\"gamma\": 0.999},\n",
    "    # log\n",
    "    # log_permutation=True, # uncomment this line to log the permutation matrix\n",
    "    # log_permutation_freq=1,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup trainer and fit the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                   | Params\n",
      "-------------------------------------------------\n",
      "0 | model | SinkhornOrderDiscovery | 46.6 K\n",
      "-------------------------------------------------\n",
      "46.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.6 K    Total params\n",
      "0.186     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9729b4168f945c3a74c1d784e3d1d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c99d2643b3f4620920973af3814c8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "# set callbacks for the trainer\n",
    "callbacks = [\n",
    "    # monitor the learning rate (log to tensorboard)\n",
    "    lightning.pytorch.callbacks.LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "]\n",
    "\n",
    "trainer = lightning.Trainer(\n",
    "    # accelerator=\"mps\",  # remove this line to run on CPU\n",
    "    callbacks=callbacks,\n",
    "    # precision=16, # for mixed precision training\n",
    "    # gradient_clip_val=1.0,\n",
    "    # gradient_clip_algorithm=\"value\",\n",
    "    track_grad_norm=\"inf\",\n",
    "    log_every_n_steps=1,\n",
    "    # detect_anomaly=True,\n",
    "    max_epochs=100,\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.fit(tm, dm)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune the learned model to remove redundant edges using observational data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 53.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of edges in the original DAG is: 8\n",
      "Structural hamming distance between pruned_dag and original_dag is: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ground_truth = dm.datasets[0].dag  # ground truth DAG\n",
    "\n",
    "pruned_dag = prune(\n",
    "    ordering=tm.get_ordering(),  # learned ordering\n",
    "    data=dm.datasets[0].samples,  # observational data\n",
    "    method=PruningMethod.CONDITIONAL_INDEPENDENCE_TESTING,  # pruning method\n",
    "    verbose=1,  # print progress\n",
    "    method_params=dict(threshold=0.05),  # confidence threshold for conditional independence testing\n",
    ")\n",
    "\n",
    "print(\"The number of edges in the original DAG is:\", np.sum(ground_truth).astype(int))\n",
    "print(\"Structural hamming distance between pruned_dag and original_dag is:\", shd(pruned_dag, ground_truth))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further prune using interventional data\n",
    "As mentioned in the [pruning notebook](./pruning.ipynb) we can further prune the model using interventional data. We should just make sure that we do not prune the parents of the intervened variable.\n",
    "As we are dealing with unknown interventions, we can treat the node with minimum average log-likelihood over the whole interventional episode (dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 353.37it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 396.63it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 363.57it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 349.67it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 412.26it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 575.51it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 812.85it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 1092.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misclassification of nodes under intervention: 3\n",
      "The number of edges in the original DAG is:  8\n",
      "Structural hamming distance between pruned_dag and original_dag is:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ground_truth = dm.datasets[0].dag\n",
    "count_incorrect = 0\n",
    "# get the interventional dataframes with the values of the intervention\n",
    "for intervention_eposide in dm.datasets[1:]:\n",
    "    intervention_node_name = intervention_eposide.intervention_node\n",
    "    # check the index of the intervention node in intervention_eposide.samples.columns\n",
    "    gt_intervention_node_index = intervention_eposide.samples.columns.get_loc(intervention_node_name)\n",
    "    # find the node being intervened on (it should have the lowest log_prob in the episode)\n",
    "    dataloader = torch.utils.data.DataLoader(intervention_eposide, batch_size=32, shuffle=False)\n",
    "\n",
    "    # find the node with the lowest log_prob on average over the episode\n",
    "    log_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(tm.device)\n",
    "            processed_batch = tm.process_batch(batch).to(tm.device)\n",
    "            logits = tm.model(processed_batch)\n",
    "            log_probs.append(log_prob(logits, in_features, batch, reduce=False))\n",
    "\n",
    "    # index of the node with the lowest log_prob\n",
    "    intervention_node_index = torch.cat(log_probs).mean(0).argmin().item()\n",
    "\n",
    "    # count the number of misclassifications\n",
    "    count_incorrect = count_incorrect + (intervention_node_index != gt_intervention_node_index)\n",
    "\n",
    "    # prune the resulting DAG\n",
    "    pruned_dag = prune(\n",
    "        ordering=tm.get_ordering(),  # learned ordering\n",
    "        data=intervention_eposide.samples,  # interventional data\n",
    "        method=PruningMethod.CONDITIONAL_INDEPENDENCE_TESTING,  # pruning method\n",
    "        dag=pruned_dag,  # DAG to prune (result of previous prunings)\n",
    "        interventional_column=intervention_node_index,  # index of the node being intervened on\n",
    "        verbose=1,  # print progress\n",
    "        method_params=dict(threshold=0.05),  # confidence threshold for conditional independence testing\n",
    "    )\n",
    "\n",
    "print(\"Number of misclassification of nodes under intervention:\", count_incorrect)\n",
    "print(\"The number of edges in the original DAG is: \", np.sum(ground_truth).astype(int))\n",
    "print(\"Structural hamming distance between pruned_dag and original_dag is: \", shd(pruned_dag, ground_truth))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a2f498358c3663e2e3193e62762a23cd9e21c3899e1ed9b7a4dc1ddd6985d97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
