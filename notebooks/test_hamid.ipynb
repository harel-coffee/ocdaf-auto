{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9167, 0.7454, 0.7233, 0.3633, 1.2489])\n",
      "tensor([[0.9167, 0.7454, 0.7233, 0.3633, 1.2489],\n",
      "        [0.6330, 0.8856, 0.3111, 0.7023, 0.8895],\n",
      "        [1.0559, 1.2179, 0.3244, 0.4147, 0.9810],\n",
      "        [1.9729, 1.2073, 1.6316, 0.5023, 1.5721],\n",
      "        [0.7000, 2.0285, 2.5598, 0.3901, 1.2406],\n",
      "        [1.1399, 1.7895, 2.6397, 1.3312, 0.7102],\n",
      "        [1.2733, 1.1020, 0.9281, 0.8327, 1.7330],\n",
      "        [0.7740, 1.8661, 1.5178, 0.7077, 1.4917],\n",
      "        [1.4559, 1.0026, 1.0521, 1.1471, 0.6872],\n",
      "        [1.6537, 1.3633, 1.3486, 1.4246, 0.6920]])\n",
      "tensor([[0.9167, 0.7454, 0.7233, 0.3633, 1.2489],\n",
      "        [0.6365, 1.0140, 0.7080, 0.3677, 0.9087],\n",
      "        [0.7536, 1.2165, 0.8818, 0.3701, 1.1156],\n",
      "        [1.4021, 1.7697, 1.6316, 0.3943, 1.9966],\n",
      "        [1.1964, 1.4657, 1.3826, 0.3901, 1.6937],\n",
      "        [1.0231, 1.7824, 1.5434, 0.3916, 1.6141],\n",
      "        [1.2823, 1.4363, 1.2928, 0.3803, 1.7330],\n",
      "        [1.0457, 1.5934, 1.2621, 0.3875, 1.4917],\n",
      "        [0.9224, 1.0360, 0.7995, 0.3746, 1.3252],\n",
      "        [1.1066, 1.1538, 1.0776, 0.3604, 1.6514]])\n",
      "tensor([0.9167, 0.7454, 0.7233, 0.3633, 1.2489])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# create a  lower triangular matrix \n",
    "n =  5\n",
    "mask = torch.tril(torch.ones(n, n))\n",
    "weight = torch.rand(n, n)\n",
    "# create a permutation matrix of size n x n\n",
    "all_perms = []\n",
    "for _ in range(10):\n",
    "    perm = torch.zeros((n, n))\n",
    "    perm[torch.arange(n), torch.randperm(n)] =  1\n",
    "    all_perms.append(perm)\n",
    "P = torch.stack(all_perms)\n",
    "# print(P)\n",
    "\n",
    "# permute the mask using the permutation matrix\n",
    "P_inv = P.transpose(1, 2)\n",
    "mask = P @ mask.repeat(10, 1, 1) @ P_inv\n",
    "\n",
    "# print(weight)\n",
    "# print(mask * weight)\n",
    "x = torch.rand(10, n)\n",
    "bias = torch.rand(n)\n",
    "# print(\"x = \", x)\n",
    "# print(\"bias = \", bias)\n",
    "# print(\"mask = \", mask)\n",
    "# print(x[None, :])\n",
    "print((mask * weight)[0] @ x[0] + bias)\n",
    "# print(x[:, :, None])\n",
    "print(torch.bmm((mask * weight), x[:, :, None]).squeeze() + bias)\n",
    "# print((mask * weight) @ x)\n",
    "print(((mask * weight) @ x.T + bias[:, None]).transpose(1, 2)[0])\n",
    "\n",
    "print((mask[0,:,:] * weight) @ x[0, :].T + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning\n",
    "import lightning.pytorch.callbacks\n",
    "from ocd.training import OrderedTrainingModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "# set callbacks for the trainer\n",
    "callbacks = [\n",
    "    # monitor the learning rate (log to tensorboard)\n",
    "    lightning.pytorch.callbacks.LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "]\n",
    "\n",
    "trainer = lightning.Trainer(\n",
    "    # accelerator=\"mps\",  # remove this line to run on CPU\n",
    "    callbacks=callbacks,\n",
    "    # precision=16, # for mixed precision training\n",
    "    # gradient_clip_val=1.0,\n",
    "    # gradient_clip_algorithm=\"value\",\n",
    "    max_epochs=10000,\n",
    "    track_grad_norm=\"inf\",\n",
    "    log_every_n_steps=1,\n",
    "    # overfit_batches=3,\n",
    "    # detect_anomaly=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Import <asia>\n",
      "[bnlearn] >Loading bif file </home/hamidreza/Work/myprojects/ocd/venv/lib/python3.10/site-packages/bnlearn/data/asia.bif>\n",
      "[bnlearn] >Check whether CPDs sum up to one.\n",
      "[bnlearn] >Check whether CPDs associated with the nodes are consistent: True\n"
     ]
    }
   ],
   "source": [
    "# setup data\n",
    "from ocd.data import CausalDataModule\n",
    "import dycode\n",
    "import torch\n",
    "\n",
    "dycode.register_context(torch)\n",
    "\n",
    "dm = CausalDataModule(\n",
    "    name=\"asia\",  # small dataset asia\n",
    "    observation_size=4096,  # number of observation samples\n",
    "    intervention_size=256,  # set to 0 for no intervention\n",
    "    batch_size=128,\n",
    "    num_workers=0,  # set to 0 for no multiprocessing\n",
    "    val_size=0,  # 10% of data for validation, or use int for exact number of samples, set to 0 for no validation\n",
    "    pin_memory=True,  # set to True for faster data transfer to GPU (if available)\n",
    ")\n",
    "dm.setup(\"fit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set torch.anomaly_detection(True) to debug\n",
    "import torch\n",
    "\n",
    "# Extract the category sizes\n",
    "in_features = dm.train_data[0].features_values\n",
    "\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "tm = OrderedTrainingModule(\n",
    "    in_covariate_features=in_features,\n",
    "    hidden_features_per_covariate=[\n",
    "        [128 for i in range(len(in_features))],\n",
    "        [64 for i in range(len(in_features))],\n",
    "        [32 for i in range(len(in_features))],\n",
    "    ],\n",
    "    bias=False,\n",
    "    batch_norm=False,\n",
    "    criterion_args=dict(\n",
    "        terms=[\n",
    "            \"ocd.training.terms.OrderedLikelihoodTerm\",\n",
    "            dict(\n",
    "                name=\"norm(gamma)\",\n",
    "                term_function='lambda training_module: training_module.model.Gamma.norm(float(\"inf\"))',\n",
    "                factor=0,\n",
    "            ),\n",
    "            dict(\n",
    "                name=\"norm(layers)\",\n",
    "                term_function='lambda training_module: max([layer.linear.weight.norm(float(\"inf\")) for layer in training_module.model.made.layers])',\n",
    "                factor=0,\n",
    "            )\n",
    "            # dict(\n",
    "            #     name='nothing',\n",
    "            #     term_function='def term(training_module, batch):\\n\\ttraining_module.batch=batch\\n\\treturn torch.zeros(1, device=batch.device)',\n",
    "            #     factor=0,\n",
    "            # )\n",
    "        ]\n",
    "    ),\n",
    "    optimizer=['torch.optim.Adam', 'torch.optim.Adam'],\n",
    "    optimizer_parameters=['model.made', 'model.Gamma'],\n",
    "    optimizer_args=[\n",
    "        dict(\n",
    "            weight_decay=0.0001,\n",
    "        ),\n",
    "        dict()\n",
    "    ],\n",
    "    optimizer_is_active=[\n",
    "        'lambda training_module: training_module.current_epoch % 10 < 10',\n",
    "        'lambda training_module: training_module.current_epoch % 10 < 10',\n",
    "    ],\n",
    "    tau_scheduler=\"lambda training_module: max(0.003, 0.5 * 0.98 ** (training_module.current_epoch // 1))\",\n",
    "    n_sinkhorn_scheduler=\"lambda training_module: min(60, max(20, 20 + ((training_module.current_epoch - 20) // 10)))\",\n",
    "    lr=0.001,\n",
    "    scheduler=\"torch.optim.lr_scheduler.ExponentialLR\",\n",
    "    scheduler_interval=\"epoch\",\n",
    "    scheduler_args={\"gamma\": 0.999},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Import <asia>\n",
      "[bnlearn] >Loading bif file </home/hamidreza/Work/myprojects/ocd/venv/lib/python3.10/site-packages/bnlearn/data/asia.bif>\n",
      "[bnlearn] >Check whether CPDs sum up to one.\n",
      "[bnlearn] >Check whether CPDs associated with the nodes are consistent: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type                   | Params\n",
      "-------------------------------------------------\n",
      "0 | model | SinkhornOrderDiscovery | 675 K \n",
      "-------------------------------------------------\n",
      "675 K     Trainable params\n",
      "0         Non-trainable params\n",
      "675 K     Total params\n",
      "2.704     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745c42ef607643dfbe06cbccfb85eec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a791c3bd7f64a1f866540d28ff04b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 20it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(tm, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(dm.trainer.datamodule.datasets[2].dag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52095eaac6f7430dcd770b4bb1719550039f99e3c10e23775974db5ba0b67989"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
