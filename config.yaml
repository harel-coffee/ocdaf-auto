# lightning.pytorch==1.9.4
seed_everything: 100
trainer:
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      name: dummy-config-without-mask-correct-order
      save_dir: .
      version: null
      offline: false
      dir: null
      id: null
      anonymous: null
      project: temporary
      log_model: false
      experiment: null
      prefix: ''
      checkpoint_name: null
      job_type: null
      config: null
      entity: null
      reinit: null
      tags: null
      group: null
      notes: null
      magic: null
      config_exclude_keys: null
      config_include_keys: null
      mode: null
      allow_val_change: null
      resume: null
      force: null
      tensorboard: null
      sync_tensorboard: null
      monitor_gym: null
      save_code: null
      settings: null
  enable_checkpointing: false
  callbacks: null
  default_root_dir: null
  gradient_clip_val: 1.0
  gradient_clip_algorithm: value
  num_nodes: 0
  num_processes: null
  devices: 1
  gpus: null
  auto_select_gpus: null
  tpu_cores: null
  ipus: null
  enable_progress_bar: false
  overfit_batches: 0.0
  track_grad_norm: -1
  check_val_every_n_epoch: 1
  fast_dev_run: false
  accumulate_grad_batches: null
  max_epochs: 150
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  val_check_interval: null
  log_every_n_steps: 1
  accelerator: cpu
  strategy: null
  sync_batchnorm: false
  precision: 32
  enable_model_summary: true
  num_sanity_val_steps: 2
  resume_from_checkpoint: null
  profiler: null
  benchmark: null
  deterministic: null
  reload_dataloaders_every_n_epochs: 0
  auto_lr_find: false
  replace_sampler_ddp: true
  detect_anomaly: false
  auto_scale_batch_size: false
  plugins: null
  amp_backend: null
  amp_level: null
  move_metrics_to_cpu: false
  multiple_trainloader_mode: max_size_cycle
  inference_mode: true
ckpt_path: null
data:
  class_path: lightning_toolbox.DataModule
  init_args:
    dataset: ocd.data.SyntheticOCDDataset
    dataset_args:
      seed: 100
      scm_generator: ocd.data.scm.InvertibleModulatedGaussianSCMGenerator
      scm_generator_args:
        graph_generator: ocd.data.scm.GraphGenerator
        graph_generator_args:
          enforce_ordering:
          - 2
          - 0
          - 1
          graph_type: random_dag
          n: 3
          p: 0.5
          m: 10
          seed: 666
        seed: 100
        std: 1.0
        mean: 0.0
        weight_s:
        - 0.5
        - 1.0
        weight_t:
        - 0.5
        - 1.0
        s_function: 'lambda x: numpy.log(1 + numpy.exp(x))

          '
        s_function_signature: softplus
        t_function: 'lambda x: numpy.log(1 + numpy.exp(x))

          '
        t_function_signature: softplus
      observation_size: 10240
    train_dataset: null
    train_dataset_args: null
    val_size: 0.1
    val_dataset: null
    val_dataset_args: null
    test_dataset: null
    test_dataset_args: null
    batch_size: 512
    train_batch_size: null
    val_batch_size: null
    test_batch_size: null
    pin_memory: true
    train_pin_memory: null
    val_pin_memory: null
    test_pin_memory: null
    train_shuffle: true
    val_shuffle: false
    test_shuffle: false
    num_workers: 0
    train_num_workers: null
    val_num_workers: null
    test_num_workers: null
    seed: 0
model:
  class_path: lightning_toolbox.TrainingModule
  init_args:
    model: null
    model_cls: ocd.models.ocdaf.OCDAF
    model_args:
      base_distribution: torch.distributions.Normal
      base_distribution_args:
        loc: 0.0
        scale: 1.0
      in_features: 3
      layers:
      - 3
      - 3
      num_transforms: 3
      additive: false
      elementwise_perm: true
      ordering:
      - 2
      - 0
      - 1
      residual: false
      bias: true
      activation: torch.nn.LeakyReLU
      activation_args:
        negative_slope: 0.1
    objective: null
    objective_cls: lightning_toolbox.Objective
    objective_args:
      nll: 'lambda training_module, batch: -training_module.model(batch)[''log_prob''].mean()

        '
    optimizer: torch.optim.Adam
    optimizer_frequency: null
    optimizer_is_active: null
    optimizer_parameters: null
    optimizer_args: null
    lr: 0.001
    scheduler: null
    scheduler_name: null
    scheduler_optimizer: null
    scheduler_args: null
    scheduler_interval: epoch
    scheduler_frequency: 1
    scheduler_monitor: null
    scheduler_strict: null
    save_hparams: true
    initialize_superclass: true
