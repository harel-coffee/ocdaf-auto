"""
This is a callback designed to save the permutations generated by the permutation learning model
it will create a file that contains some statistics on the correct permutation.
"""
from lightning.pytorch.callbacks import Callback
import os
from lightning_toolbox import TrainingModule
from lightning.pytorch import Trainer
import typing as th
import numpy as np
import json
import networkx as nx
import ocd.evaluation as eval_metrics
import dypy as dy
import warnings
from ocd.post_processing import pc_based_pruning
from causallearn.search.ConstraintBased.PC import pc
import functools  
import pandas as pd
from causallearn.utils.cit import kci, gsq
import networkx as nx

skeleton = None

def my_optimized_shd(perm, dag, df: pd.DataFrame):
    """
    This is a modified version of the shd function that 
    is optimized for the permutation learning model logs.
    """
    global skeleton
    if skeleton is None:
        # First time to calculate the skeleton
        df_np = df.to_numpy()
        cg = pc(df_np, 0.05, gsq, verbose=False)
        graph = cg.G.graph
        skeleton = set()
        for i, v in enumerate(df.columns):
            for j, u in enumerate(df.columns):
                if i >= j:
                    continue
                u_idx = df.columns.get_loc(u)
                v_idx = df.columns.get_loc(v)
                if graph[u_idx, v_idx] != 0 or graph[v_idx, u_idx] != 0:
                    skeleton.add((v, u))
                    skeleton.add((u, v))
    all_edges = []
    for i in range(len(perm)):
        for j in range(i+1, len(perm)):
            if (perm[i], perm[j]) in skeleton:
                all_edges.append((perm[i], perm[j]))
    # Create an nx.Digraph with nodes perm and edges all_edges
    predicted_graph = nx.DiGraph()
    predicted_graph.add_nodes_from(perm)
    predicted_graph.add_edges_from(all_edges)
    # print("original dag", dag)
    # print("predicted", predicted_graph)
    return eval_metrics.shd(predicted_graph, dag)
            
all_evaluation_metrics = {
    "backward_relative_penalty": eval_metrics.backward_relative_penalty,
    "count_backward": eval_metrics.count_backward,
    "posterior_parent_ratio": eval_metrics.posterior_parent_ratio,
    "pc-shd": my_optimized_shd,
}


class SavePermutationResultsCallback(Callback):
    def __init__(
        self,
        save_path: th.Optional[str] = None,
        save_every_n_epochs: th.Optional[int] = None,
        log_every_n_epochs: th.Optional[int] = None,
        num_samples: int = 1000,
        log_into_logger: bool = True,
        evaluation_metrics: th.Optional[th.List[str]] = None,
        ignore_evaluation_metrics: th.Optional[th.List[str]] = ['pc-shd'],
    ):
        if save_path is None:
            # TODO: This can have side-effects
            # send a warning that the save_path is not set
            # and that the results will not be saved
            warnings.warn("save_path is not set, results will not be saved [This might cause issues]")
            return
        self.save_path = save_path
        print(">>>>", self.save_path)
        # create save_path if it does not exist
        if not os.path.exists(save_path):
            os.makedirs(save_path)

        self.save_every_n_epochs = save_every_n_epochs
        self.log_every_n_epochs = log_every_n_epochs
        self.epoch_counter = 0
        self.num_samples = num_samples

        self.log_into_logger = log_into_logger
        
        self.evaluation_metrics = {}
        if evaluation_metrics is not None:
            for metric_name in evaluation_metrics:
                self.evaluation_metrics[metric_name] = all_evaluation_metrics[metric_name]
        else:
            self.evaluation_metrics = all_evaluation_metrics
        
        if ignore_evaluation_metrics is not None:
            for metric_name in ignore_evaluation_metrics:
                self.evaluation_metrics.pop(metric_name, None)
            
    def on_train_start(self, trainer: Trainer, pl_module: TrainingModule) -> None:
        # Save the causal graph in the callback from the corresponding datamodule
        self.causal_graph = trainer.datamodule.data.dag
    
    def _get_res_dict(self, pl_module: TrainingModule):
        # save the results
        perm_model = pl_module.model.permutation_model

        all_permutations = perm_model(
            self.num_samples, return_matrix=False, permutation_type="hard", training_module=pl_module
        )["perm_mat"]

        permutation_map = {}
        unique_permutations, counts = np.unique(all_permutations, axis=0, return_counts=True)
        mx = None
        for perm, c in zip(unique_permutations, counts):
            key = "-".join([str(i) for i in perm])
            permutation_map[key] = 1.0 * c / np.sum(counts)
            if mx is None or permutation_map[key] > permutation_map[mx]:
                mx = key

        best_permutation = [int(i) for i in mx.split("-")]
        ret = {}
        ret["metrics"] = {"average": {}, "best": {}}

        # Calculate all the metrics
        for metric_name, metric_func in self.evaluation_metrics.items():
            sm = 0
            running_avg = 0
            for perm, c in permutation_map.items():
                perm_int = [int(i) for i in perm.split("-")]
                score = metric_func(perm_int, self.causal_graph)
                running_avg = (running_avg * sm + score * c) / (sm + c)
                sm += c
            ret["metrics"]["average"][metric_name] = running_avg
            ret["metrics"]["best"][metric_name] = metric_func(perm=best_permutation, dag=self.causal_graph)

        ret["permutation_map"], ret["most_common_permutation"] = permutation_map, mx

        return ret

    def _save_results(self, pl_module: TrainingModule, filename: th.Optional[str] = None) -> None:
        filename = filename if filename is not None else f"results-epoch-{self.epoch_counter}"
        filename += ".json"

        ret = self._get_res_dict(pl_module)
        saving_path = os.path.join(self.save_path, filename)
        # save res to saving_path
        with open(saving_path, "w") as f:
            json.dump(ret, f, indent=4)

    def _log_results(self, pl_module: TrainingModule) -> None:
        ret = self._get_res_dict(pl_module)
        for key1, val1 in ret["metrics"].items():
            for key2, val2 in val1.items():
                pl_module.log(f"metrics/{key1}-{key2}", float(val2))

    def on_fit_end(self, trainer: Trainer, pl_module: TrainingModule) -> None:
        self._save_results(pl_module, filename="final-results")
        return super().on_fit_end(trainer, pl_module)
    
    def on_fit_start(self, trainer: Trainer, pl_module: TrainingModule) -> None:
        # Setup the skeleton
        if 'pc-shd' in self.evaluation_metrics:
            self.evaluation_metrics['pc-shd'] = functools.partial(self.evaluation_metrics['pc-shd'], df=trainer.datamodule.data.samples)
        return super().on_fit_start(trainer, pl_module)
    
    def on_train_epoch_end(self, trainer: Trainer, pl_module: TrainingModule) -> None:
        self.epoch_counter += 1
        if self.save_every_n_epochs is not None and self.epoch_counter % self.save_every_n_epochs == 0:
            # save the results
            self._save_results(pl_module)
        if self.log_every_n_epochs is not None and self.epoch_counter % self.log_every_n_epochs == 0:
            self._log_results(pl_module)

        return super().on_train_epoch_end(trainer, pl_module)
