# callbacks
callbacks:
  - class_path: lightning.pytorch.callbacks.lr_monitor.LearningRateMonitor
    init_args:
      logging_interval: step
  - class_path: ocd.training.callbacks.data_visualizer.DataVisualizer
  - class_path: ocd.training.callbacks.phase_changer.PhaseChangerCallback
    init_args:
      starting_phase: maximization
      # Set to higher value for faster results
      check_every_n_iterations: 1
      # The settings regarding epoch limit values
      maximization_epoch_limit: 100
      expectation_epoch_limit: 100
      # The settings regarding the generalization gap
      generalization_early_stopping: []
      generalization_patience: 10
      generalization_threshold_eps: 0.01
  - class_path: ocd.training.callbacks.birkhoff_visualizer.BirkhoffCallback
    init_args:
      epoch_buffer_size: 1
      evaluate_every_n_epochs: 5
      permutation_size: 3
      seed: 666
      write_cost_values: True
      write_permutation_names: True
      core_points_has_birkhoff_vertices: True
      core_points_has_birkhoff_edges: False
      reject_outlier_factor: 0.1
      # Including correct orderings
      causal_graph:
        class_path: networkx.classes.digraph.DiGraph
        init_args:
          incoming_graph_data:
            - [2, 0]
            - [2, 1]
            - [0, 1]
      # Include permutation names
      add_permutation_to_name: True
  - class_path: ocd.training.callbacks.save_results.SavePermutationResultsCallback
    init_args:
      save_path: experiments/dummy/trainer-3-digraph
      save_every_n_epochs: 20
      num_samples: 1000
      # Including correct orderings
      causal_graph:
        class_path: networkx.classes.digraph.DiGraph
        init_args:
          incoming_graph_data:
            - [2, 0]
            - [2, 1]
            - [0, 1]
     
# device
accelerator: cpu
devices: 1
num_nodes: 0
# optimization
gradient_clip_algorithm: value
gradient_clip_val: 1.0
# precision: 16
# limits & training loop controls
max_epochs: 1000
log_every_n_steps: 1
check_val_every_n_epoch: 1
# tracking & debugging
# track_grad_norm: inf
# logger is setup while initializing the trainer
enable_checkpointing: true
enable_model_summary: true
enable_progress_bar: false
logger:
  class_path: lightning.pytorch.loggers.WandbLogger
  init_args:
    project: temporary-permutation
    name: good-data-run-softplus
    checkpoint_name: full-pipeline
