class_path: lightning_toolbox.TrainingModule
init_args:
  # the model
  model_cls: ocd.models.ocdaf.OCDAF
  model_args:
    # Permutation learner arguments
    use_permutation: True
    permutation_learner_cls: ocd.models.permutation.LearnablePermutation
    permutation_learner_args:
      gumbel_noise_std: >
        lambda self, training_module, **kwargs: 0.5
    # Base distribution arguments
    base_distribution: torch.distributions.Normal
    base_distribution_args:
      loc: 0.0
      scale: 1.0
    # The flow model arguments
    in_features: 3
    layers: [33, 99, 33]
    num_transforms: 1
    additive: False
    residual: False
    bias: true
    activation: torch.nn.LeakyReLU
    activation_args:
      negative_slope: 0.1
    clamp_val: 10000000
  # the optimizer
  optimizer: [torch.optim.AdamW, torch.optim.Adam]
  optimizer_parameters:
    - model.flow
    - model.permutation_model
  optimizer_is_active: 
    - > 
      lambda training_module: training_module.current_phase == 'maximization'
    - >
      lambda training_module: training_module.current_phase == 'expectation'

  lr: [0.0001, 0.0001]
  # scheduler: [torch.optim.lr_scheduler.ExponentialLR, torch.optim.lr_scheduler.ExponentialLR]
  # scheduler_args:
  #   - gamma: 0.75
  #     # factor: 0.75
  #     # patience: 10
  #   - gamma: 0.75
  #     # factor: 0.75
  #     # patience: 10
  # scheduler_name: ["lr_scheduler_maximization", "lr_scheduler_expectation"]
  # scheduler_interval: ["epoch", "epoch"]
  # scheduler_frequency: [22500, 22500]
  # scheduler_monitor: ["loss/val", "loss/val"]
  # the loss
  # objective_cls: ocd.training.terms.LikelihoodTerm
  objective_args:
    nll: >
      lambda training_module, batch: -training_module.forward(batch)['log_prob'].mean()
