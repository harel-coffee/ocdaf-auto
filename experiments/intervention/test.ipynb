{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/vahidzee/.pyenv/versions/3.10.9/envs/deep/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingModule(\n",
       "  (model): OCDAF(\n",
       "    (flow): AffineFlow(\n",
       "      (0): MaskedAffineFlowTransform(\n",
       "        ordering=tensor([23,  5,  8,  3, 11, 18, 22, 16, 10,  7,  6, 17,  4,  1, 12, 13, 20,  9,\n",
       "                19, 15,  0, 14, 21,  2, 24], dtype=torch.int32)\n",
       "        (masked_mlp_shift): MaskedMLP(\n",
       "          num_masks=1, seed=0\n",
       "          (0): MaskedBlock(\n",
       "            in_features=25, out_features=625, bias=True\n",
       "            (activation): LeakyReLU(negative_slope=0.1)\n",
       "          )\n",
       "          (1): MaskedBlock(\n",
       "            in_features=625, out_features=625, bias=True\n",
       "            (activation): LeakyReLU(negative_slope=0.1)\n",
       "          )\n",
       "          (2): MaskedBlock(\n",
       "            in_features=625, out_features=625, bias=True\n",
       "            (activation): LeakyReLU(negative_slope=0.1)\n",
       "          )\n",
       "          (3): MaskedBlock(in_features=625, out_features=25, bias=True)\n",
       "        )\n",
       "        (masked_mlp_scale): MaskedMLP(\n",
       "          num_masks=1, seed=0\n",
       "          (0): MaskedBlock(\n",
       "            in_features=25, out_features=625, bias=True\n",
       "            (activation): LeakyReLU(negative_slope=0.1)\n",
       "          )\n",
       "          (1): MaskedBlock(\n",
       "            in_features=625, out_features=625, bias=True\n",
       "            (activation): LeakyReLU(negative_slope=0.1)\n",
       "          )\n",
       "          (2): MaskedBlock(\n",
       "            in_features=625, out_features=625, bias=True\n",
       "            (activation): LeakyReLU(negative_slope=0.1)\n",
       "          )\n",
       "          (3): MaskedBlock(in_features=625, out_features=25, bias=True)\n",
       "        )\n",
       "        (scale_transform): ScaleTransform(\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import lightning_toolbox as ltb\n",
    "import lightning\n",
    "import dypy as dy\n",
    "from ocd.models.permutation.utils import all_permutations\n",
    "import sys\n",
    "import ocd\n",
    "import random\n",
    "\n",
    "# create a random permutation of n numbers\n",
    "\n",
    "data_config = yaml.safe_load(\n",
    "    open(\"experiments/intervention/data/parametric_laplace_modulated_25_1000_fork_sin_plus_x.yml\", \"r\")\n",
    ")\n",
    "model_config = yaml.safe_load(open(\"experiments/intervention/model.yml\", \"r\"))\n",
    "\n",
    "n = data_config[\"init_args\"][\"dataset_args\"][\"scm_generator_args\"][\"graph_generator_args\"][\"n\"]\n",
    "model_config[\"init_args\"][\"model_args\"][\"in_features\"] = n\n",
    "\n",
    "# setup fixed perm\n",
    "enforce_perm = torch.randperm(n).tolist()\n",
    "data_config[\"init_args\"][\"dataset_args\"][\"scm_generator_args\"][\"graph_generator_args\"][\n",
    "    \"enforce_ordering\"\n",
    "] = enforce_perm\n",
    "model_config[\"init_args\"][\"model_args\"][\"ordering\"] = enforce_perm\n",
    "model_config[\"init_args\"][\"model_args\"]['scale_transform_args'] = dict(post_act_scale=5.0, normalization=None)\n",
    "trainer_config = yaml.safe_load(open(\"experiments/intervention/trainer.yml\", \"r\"))\n",
    "trainer_config[\"max_epochs\"] = 400\n",
    "trainer_config[\"enable_progress_bar\"] = False\n",
    "trainer_config[\"enable_model_summary\"] = True\n",
    "grad_clip_val = 1000\n",
    "\n",
    "dm = ltb.DataModule(**data_config[\"init_args\"])\n",
    "model = dy.eval(model_config[\"class_path\"])(**model_config[\"init_args\"])\n",
    "\n",
    "trainer = lightning.Trainer(\n",
    "    **trainer_config,\n",
    "    callbacks=[lightning.pytorch.callbacks.LearningRateMonitor()],\n",
    "    gradient_clip_val=1.0,\n",
    "    gradient_clip_algorithm='value',\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type  | Params\n",
      "--------------------------------\n",
      "0 | model | OCDAF | 1.6 M \n",
      "--------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.515     Total estimated model params size (MB)\n",
      "/Users/vahidzee/.pyenv/versions/3.10.9/envs/deep/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/vahidzee/.pyenv/versions/3.10.9/envs/deep/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/vahidzee/.pyenv/versions/3.10.9/envs/deep/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
