seed_everything: 40
trainer:
  accelerator: gpu
  strategy: auto
  devices: 1
  num_nodes: 0
  precision: 32-true
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      name: null
      save_dir: .
      version: null
      offline: false
      dir: null
      id: null
      anonymous: null
      project: smart-trainer
      log_model: false
      experiment: null
      prefix: ''
      checkpoint_name: null
  callbacks:
  - class_path: ocd.training.callbacks.data_visualizer.DataVisualizer
    init_args:
      image_size: null
      show_functions_in_dag: true
      show_original_statistics_on_histograms: true
  - class_path: ocd.training.callbacks.phase_changer.PhaseChangerCallback
    init_args:
      starting_phase: maximization
      monitor_validation: false
      monitor_training: true
      maximization_epoch_limit: 80
      expectation_epoch_limit: 55
      patience: 15
      threshold: 0.0001
      cooldown: 200
      reset_optimizers: false
      reinitialize_weights_on_maximization: false
      log_onto_logger: true
  - class_path: ocd.training.callbacks.save_results.SavePermutationResultsCallback
    init_args:
      save_path: experiments/final_runs/sweeps/synthetic-small-parametric/smart-synthetic_parametric_linear_additive_n4_laplace_SinPlusX_erdos_seed3-2023-05-16-00-52-57
      save_every_n_epochs: null
      log_every_n_epochs: 5
      num_samples: 5000
      ordering_to_score_mapping: null
      causal_graph:
        class_path: networkx.DiGraph
        init_args:
          incoming_graph_data:
          - - 0
            - 3
          - - 0
            - 2
          - - 3
            - 1
          - - 3
            - 2
          - - 1
            - 2
      log_into_logger: true
      evaluation_metrics: null
      ignore_evaluation_metrics:
      - pc-shd
  fast_dev_run: false
  max_epochs: 1
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: null
  check_val_every_n_epoch: 1
  num_sanity_val_steps: null
  log_every_n_steps: 1
  enable_checkpointing: true
  enable_progress_bar: false
  enable_model_summary: true
  accumulate_grad_batches: 1
  gradient_clip_val: null
  gradient_clip_algorithm: null
  deterministic: null
  benchmark: null
  inference_mode: true
  use_distributed_sampler: true
  profiler: null
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: null
data:
  class_path: lightning_toolbox.DataModule
  init_args:
    dataset: ocd.data.SyntheticOCDDataset
    dataset_args:
      observation_size: 1000
      standardization: true
      reject_outliers_n_far_from_mean: 5
      name: synthetic_parametric_linear_additive_n4_laplace_SinPlusX_erdos_seed3
      scm_generator: ocd.data.synthetic.ParametricSCMGenerator
      scm_generator_args:
        graph_generator: ocd.data.scm.GraphGenerator
        weight_s:
        - 0.5
        - 1.5
        weight_t:
        - 0.5
        - 1.5
        s_function:
          function_descriptor: "def func(x):\n  return numpy.ones_like(x)\n"
          function_of_interest: func
        s_function_signature: one
        graph_generator_args:
          n: 4
          graph_type: erdos_renyi
          p: 0.4
          seed: 44
        noise_type: laplace
        noise_parameters:
          loc:
          - 0.0
          - 0.0
          scale:
          - 1.0
          - 1.0
        t_function:
          function_descriptor: "def func(x):\n  return numpy.sin(x) + x\n"
          function_of_interest: func
        t_function_signature: sin_plus_x
        seed: 43
      seed: 41
    train_dataset: null
    train_dataset_args: null
    val_size: 0.01
    val_dataset: null
    val_dataset_args: null
    test_dataset: null
    test_dataset_args: null
    transforms: null
    train_transforms: null
    val_transforms: null
    test_transforms: null
    batch_size: 128
    train_batch_size: null
    val_batch_size: null
    test_batch_size: null
    pin_memory: true
    train_pin_memory: null
    val_pin_memory: null
    test_pin_memory: null
    train_shuffle: true
    val_shuffle: false
    test_shuffle: false
    num_workers: 0
    train_num_workers: null
    val_num_workers: null
    test_num_workers: null
    seed: 0
    save_hparams: true
    initialize_superclass: true
model:
  class_path: ocd.training.module.OCDafTrainingModule
  init_args:
    maximization_specifics: null
    expectation_specifics: null
    grad_clip_val: 1.0
    phases:
    - maximization
    - expectation
    model: null
    model_cls: ocd.models.ocdaf.OCDAF
    model_args:
      use_permutation: true
      permutation_learner_cls: ocd.models.permutation.LearnablePermutation
      permutation_learner_args:
        gumbel_noise_std: 'lambda self, training_module, **kwargs: 2 - (2 / (training_module.trainer.max_epochs))
          * (training_module.current_epoch)

          '
      base_distribution: torch.distributions.Normal
      base_distribution_args:
        loc: 0.0
        scale: 1.0
      layers:
      - 10
      - 5
      - 5
      populate_features: true
      layers_limit:
      - 100
      - 300
      - 50
      num_transforms: 1
      additive: false
      residual: false
      bias: true
      activation: torch.nn.LeakyReLU
      activation_args:
        negative_slope: 0.1
      scale_transform: true
      scale_transform_s_args:
        pre_act_scale: 0.4
        post_act_scale: 5.0
      scale_transform_t_args:
        pre_act_scale: 0.1
        post_act_scale: 10.0
      in_features: 4
    objective: null
    objective_cls: lightning_toolbox.Objective
    objective_args:
      nll:
        code: "def func(training_module, batch):\n  t = training_module.forward(batch)\n\
          \  res = t['log_prob']\n  return -res.mean()\n"
        function_of_interest: func
    optimizer:
    - torch.optim.AdamW
    - torch.optim.AdamW
    optimizer_frequency: null
    optimizer_is_active:
    - 'lambda training_module: training_module.current_phase == ''maximization'' if
      hasattr(training_module, ''current_phase'') else True

      '
    - 'lambda training_module: training_module.current_phase == ''expectation'' if
      hasattr(training_module, ''current_phase'') else True

      '
    optimizer_parameters:
    - model.flow
    - model.permutation_model
    optimizer_args:
    - weight_decay: 0.1
    - weight_decay: 0.01
    lr:
    - 0.01
    - 0.01
    scheduler:
    - torch.optim.lr_scheduler.ReduceLROnPlateau
    - torch.optim.lr_scheduler.ReduceLROnPlateau
    scheduler_name:
    - lr_scheduler_maximization
    - lr_scheduler_expectation
    scheduler_optimizer:
    - 0
    - 1
    scheduler_args:
    - mode: min
      min_lr: 5.0e-05
      threshold: 0.001
      factor: 0.5
      patience: 70
    - mode: min
      min_lr: 0.0005
      threshold: 0.001
      factor: 0.5
      patience: 55
    scheduler_interval: epoch
    scheduler_frequency: 1
    scheduler_monitor:
    - loss
    - loss
    scheduler_strict: null
    save_hparams: true
    initialize_superclass: true
