# This is a sweep on different hyper parameters for the final model that we are using
project: hyperparam-sweep
checkpoint_interval: 30
default_root_dir: experiments/sweep
use_smart_trainer: True
agent_run_args:
  count: 1
sweep_configuration:
  method: random
  metric:
    goal: minimize
    name: metrics/average-backward_relative_penalty
  parameters:
    sweep_group_scheduling_A:
      sweep_identifier: scheduling
      sweep: True
      sweep_alias:
        - epoch6000max200exp100
        - epoch6000max200exp200
        - epoch3000max100exp50
        - epoch3000max100exp100
        - epoch600max60exp60
        - epoch600max60exp30
      values:
        - trainer:
            max_epochs: 6000
            callbacks:
              sweep_list_operations:
                - sweep_overwrite: 0
              init_args:
                maximization_epoch_limit: 200
                expectation_epoch_limit: 100
                patience: 15
                cooldown: 200
          model:
            init_args:
              model_args:
                permutation_learner_args:
                  gumbel_noise_std:
                    code: |
                      def func(self, training_module, **kwargs):
                        tot_epochs_on_exp = 6000 / 300 * 100
                        step_size = 2.0 / tot_epochs_on_exp
                        if not hasattr(training_module, 'last_std'):
                          training_module.last_std = 2.0
                        if training_module.current_phase != 'maximization':
                          training_module.last_std -= step_size
                        return training_module.last_std
                    function_of_interest: func

              scheduler_args:
                - mode: min
                  factor: 0.8
                  patience: 50
                  min_lr: 0.000005
                  threshold: 0.0001
                - mode: min
                  factor: 0.8
                  patience: 50
                  min_lr: 0.000005
                  threshold: 0.0001
        - trainer:
            max_epochs: 6000
            callbacks:
              sweep_list_operations:
                - sweep_overwrite: 0
              init_args:
                maximization_epoch_limit: 200
                expectation_epoch_limit: 200
                patience: 15
                cooldown: 200
          model:
            init_args:
              model_args:
                permutation_learner_args:
                  gumbel_noise_std:
                    code: |
                      def func(self, training_module, **kwargs):
                        tot_epochs_on_exp = 6000 / 400 * 200
                        step_size = 2.0 / tot_epochs_on_exp
                        if not hasattr(training_module, 'last_std'):
                          training_module.last_std = 2.0
                        if training_module.current_phase != 'maximization':
                          training_module.last_std -= step_size
                        return training_module.last_std
                    function_of_interest: func

              scheduler_args:
                - mode: min
                  factor: 0.8
                  patience: 50
                  min_lr: 0.000005
                  threshold: 0.0001
                - mode: min
                  factor: 0.8
                  patience: 50
                  min_lr: 0.000005
                  threshold: 0.0001
        - trainer:
            max_epochs: 3000
            callbacks:
              sweep_list_operations:
                - sweep_overwrite: 0
              init_args:
                maximization_epoch_limit: 100
                expectation_epoch_limit: 50
                patience: 15
                cooldown: 100
          model:
            init_args:
              model_args:
                permutation_learner_args:
                  gumbel_noise_std:
                    code: |
                      def func(self, training_module, **kwargs):
                        tot_epochs_on_exp = 3000 / 150 * 50
                        step_size = 2.0 / tot_epochs_on_exp
                        if not hasattr(training_module, 'last_std'):
                          training_module.last_std = 2.0
                        if training_module.current_phase != 'maximization':
                          training_module.last_std -= step_size
                        return training_module.last_std
                    function_of_interest: func
              scheduler_args:
                - mode: min
                  factor: 0.8
                  patience: 30
                  min_lr: 0.000005
                  threshold: 0.0001
                - mode: min
                  factor: 0.8
                  patience: 30
                  min_lr: 0.000005
                  threshold: 0.0001
        - trainer:
            max_epochs: 3000
            callbacks:
              sweep_list_operations:
                - sweep_overwrite: 0
              init_args:
                maximization_epoch_limit: 100
                expectation_epoch_limit: 100
                patience: 15
                cooldown: 100
          model:
            init_args:
              model_args:
                permutation_learner_args:
                  gumbel_noise_std:
                    code: |
                      def func(self, training_module, **kwargs):
                        tot_epochs_on_exp = 3000 / 200 * 100
                        step_size = 2.0 / tot_epochs_on_exp
                        if not hasattr(training_module, 'last_std'):
                          training_module.last_std = 2.0
                        if training_module.current_phase != 'maximization':
                          training_module.last_std -= step_size
                        return training_module.last_std
                    function_of_interest: func
              scheduler_args:
                - mode: min
                  factor: 0.8
                  patience: 30
                  min_lr: 0.000005
                  threshold: 0.0001
                - mode: min
                  factor: 0.8
                  patience: 30
                  min_lr: 0.000005
                  threshold: 0.0001
        - trainer:
            max_epochs: 600
            callbacks:
              sweep_list_operations:
                - sweep_overwrite: 0
              init_args:
                maximization_epoch_limit: 60
                expectation_epoch_limit: 60
                patience: 15
                cooldown: 100
          model:
            init_args:
              model_args:
                permutation_learner_args:
                  gumbel_noise_std:
                    code: |
                      def func(self, training_module, **kwargs):
                        tot_epochs_on_exp = 600 / 90 * 60
                        step_size = 2.0 / tot_epochs_on_exp
                        if not hasattr(training_module, 'last_std'):
                          training_module.last_std = 2.0
                        if training_module.current_phase != 'maximization':
                          training_module.last_std -= step_size
                        return training_module.last_std
                    function_of_interest: func
              scheduler_args:
                - mode: min
                  factor: 0.8
                  patience: 10
                  min_lr: 0.000005
                  threshold: 0.0001
                - mode: min
                  factor: 0.8
                  patience: 10
                  min_lr: 0.000005
                  threshold: 0.0001
        - trainer:
            max_epochs: 600
            callbacks:
              sweep_list_operations:
                - sweep_overwrite: 0
              init_args:
                maximization_epoch_limit: 60
                expectation_epoch_limit: 30
                patience: 15
                cooldown: 100
          model:
            init_args:
              model_args:
                permutation_learner_args:
                  gumbel_noise_std:
                    code: |
                      def func(self, training_module, **kwargs):
                        tot_epochs_on_exp = 600 / 90 * 30
                        step_size = 2.0 / tot_epochs_on_exp
                        if not hasattr(training_module, 'last_std'):
                          training_module.last_std = 2.0
                        if training_module.current_phase != 'maximization':
                          training_module.last_std -= step_size
                        return training_module.last_std
                    function_of_interest: func
              scheduler_args:
                - mode: min
                  factor: 0.8
                  patience: 10
                  min_lr: 0.000005
                  threshold: 0.0001
                - mode: min
                  factor: 0.8
                  patience: 10
                  min_lr: 0.000005
                  threshold: 0.0001
    sweep_group_scheduling_B:
      sweep_identifier: gumbel_std
      sweep: True
      sweep_alias:
        - linear
        - no_change_on_maxim
      values:
        - model:
            init_args:
              model_args:
                permutation_learner_args:
                  gumbel_noise_std: >
                    lambda self, training_module, **kwargs: 2 - (2 / (training_module.trainer.max_epochs)) * (training_module.current_epoch)
        - model:
            init_args:
              model_args:
                permutation_learner_args:
                  gumbel_noise_std: sweep_as_is
    # model:
    #   init_args:
    #     model_args:
    #       sweep_group_scale_transform:
    #         sweep: True
    #         sweep_identifier: scale_transform
    #         sweep_alias:
    #           - noScale
    #           - looseScale
    #           - withScale
    #         values:
    #           - scale_transform: false
    #           - scale_transform: true
    #             scale_transform_s_args:
    #               pre_act_scale: 0.4
    #               post_act_scale: 5.
    #             scale_transform_t_args:
    #               pre_act_scale: 0.1
    #               post_act_scale: 10.
    #           - scale_transform: true
    #             scale_transform_s_args:
    #               pre_act_scale: 0.1
    #               post_act_scale: 10.
    #             scale_transform_t_args:
    #               pre_act_scale: 0.01
    #               post_act_scale: 100.

    # sweep_group_architecture:
    #   sweep_identifier: MLPArchitecture
    #   sweep: True
    #   sweep_alias:
    #     - Simple
    #     - Complex
    #   values:
    #     - layers: [5, 15, 5]
    #       populate_features: True
    #       layers_limit: [100, 300, 50]
    #     - layers: [10, 30, 10]
    #       populate_features: True
    #       layers_limit: [100, 300, 50]
    # sweep_group_weight_decay:
    #   sweep_identifier: WeightDecay
    #   sweep: True
    #   sweep_alias:
    #     - medium_weight
    #     - large_weight
    #   values:
    #     - optimizer_args:
    #         - weight_decay: 0.01
    #         - weight_decay: 0.01
    #     - optimizer_args:
    #         - weight_decay: 0.1
    #         - weight_decay: 0.01
    sweep_group_datasets:
      sweep_identifier: Dataset
      sweep: True
      sweep_alias:
        # - Sin-Erdos-25
        - NonParam-Chain-10
        - Cube-Full-5
        - Sin-Full-3
      values:
        # - data:
        #     init_args:
        #       dataset_args:
        #         name: parametric_normal_modulated_25_2048_erdos_renyi_sin_plus_x
        #         scm_generator: ocd.data.synthetic.ParametricSCMGenerator
        #         scm_generator_args:
        #           graph_generator: ocd.data.scm.GraphGenerator
        #           graph_generator_args:
        #             graph_type: erdos_renyi
        #             n: 25
        #             p: 0.4
        #             seed: 282
        #           noise_parameters: { loc: 0.0, scale: 1.0 }
        #           noise_type: normal
        #           s_function:
        #             function_descriptor: |
        #               def func(x):
        #                 x[x < 100] = numpy.log(1 + numpy.exp(x[x < 100]))
        #                 return x
        #             function_of_interest: func
        #           s_function_signature: softplus
        #           seed: 979
        #           t_function:
        #             function_descriptor: |
        #               def func(x):
        #                 return numpy.sin(x) + x
        #             function_of_interest: func
        #           t_function_signature: sin_plus_x
        #           weight_s: [0.5, 1.5]
        #           weight_t: [0.5, 1.5]
        #   model:
        #     init_args:
        #       model_args:
        #         permutation_learner_args:
        #           maximum_basis_size: 64
        - data:
            init_args:
              dataset_args:
                name: non_parametric_non_linear_gaussian_10_2048_chain
                scm_generator: ocd.data.synthetic.GaussianProcessBasedSCMGeberator
                scm_generator_args:
                  graph_generator: ocd.data.scm.GraphGenerator
                  graph_generator_args:
                    graph_type: chain
                    n: 10
                    seed: 83
                  noise_mean: 0.0
                  noise_std: 1.0
                  s_gamma_rbf_kernel: 1.0
                  s_mean_function_activation:
                    function_descriptor: |
                      def func(x):
                        x[x < 100] = numpy.log(1 + numpy.exp(x[x < 100]))
                        return x
                    function_of_interest: func
                  s_mean_function_activation_signature: softplus
                  s_mean_function_weights: [0.01, 0.1]
                  s_variance_rbf_kernel: 1.0
                  seed: 204
                  t_mean_function_activation:
                    function_descriptor: |
                      def func(x):
                        return numpy.sin(x) + x
                    function_of_interest: func
                  t_mean_function_activation_signature: sin_plus_x
                  t_mean_function_weights: [0.01, 0.1]
                  t_variance_rbf_kernel: 1.0
          model:
            init_args:
              model_args:
                scale_transform: true
                scale_transform_s_args:
                  pre_act_scale: 0.1
                  post_act_scale: 10.
                scale_transform_t_args:
                  pre_act_scale: 0.01
                  post_act_scale: 100.
                permutation_learner_args:
                  maximum_basis_size: 64
              scheduler_args:
                - patience: 100
                - patience: 100
        - data:
            init_args:
              dataset_args:
                name: parametric_normal_modulated_5_2048_full_cube_dislocate
                scm_generator: ocd.data.synthetic.ParametricSCMGenerator
                scm_generator_args:
                  graph_generator: ocd.data.scm.GraphGenerator
                  graph_generator_args:
                    graph_type: full
                    n: 5
                    seed: 136
                  noise_parameters:
                    loc: 0.0
                    scale: 1.0
                  noise_type: normal
                  s_function:
                    function_descriptor: |
                      def func(x):
                        x[x < 100] = numpy.log(1 + numpy.exp(x[x < 100]))
                        return x
                    function_of_interest: func
                  s_function_signature: softplus
                  seed: 419
                  t_function:
                    function_descriptor: |
                      def func(x):
                        x[x > 100] = 100
                        x[x < -100] = -100
                        x_mean = numpy.mean(x)
                        x_std = numpy.std(x)
                        if x_std == 0:
                          x_std = 1
                        x = (x - x_mean) / x_std
                        ret = x**3 + 6
                        return ret
                    function_of_interest: func
                  t_function_signature: cube_and_dislocate
                  weight_s: [0.5, 1.5]
                  weight_t: [0.5, 1.5]
          model:
            init_args:
              model_args:
                scale_transform: true
                scale_transform_s_args:
                  pre_act_scale: 0.1
                  post_act_scale: 10.
                scale_transform_t_args:
                  pre_act_scale: 0.01
                  post_act_scale: 100.
                permutation_learner_args:
                  maximum_basis_size: 10
        - data:
            init_args:
              dataset_args:
                name: parametric_normal_modulated_3_2048_full_sin_plus_x
                scm_generator: ocd.data.synthetic.ParametricSCMGenerator
                scm_generator_args:
                  graph_generator: ocd.data.scm.GraphGenerator
                  graph_generator_args:
                    graph_type: full
                    n: 3
                    seed: 582
                  noise_parameters:
                    loc: 0.0
                    scale: 1.0
                  noise_type: normal
                  s_function:
                    function_descriptor: |
                      def func(x):
                        x[x < 100] = numpy.log(1 + numpy.exp(x[x < 100]))
                        return x
                    function_of_interest: func
                  s_function_signature: softplus
                  seed: 780
                  t_function:
                    function_descriptor: |
                      def func(x):
                        return numpy.sin(x) + x
                    function_of_interest: func
                  t_function_signature: sin_plus_x
                  weight_s: [0.5, 1.5]
                  weight_t: [0.5, 1.5]
          model:
            init_args:
              model_args:
                permutation_learner_args:
                  maximum_basis_size: 10
                scale_transform: true
                scale_transform_s_args:
                  pre_act_scale: 0.1
                  post_act_scale: 10.
                scale_transform_t_args:
                  pre_act_scale: 0.01
                  post_act_scale: 100.
    sweep_group_seed_data:
      sweep: True
      sweep_alias:
        - seed0
        - seed1
        - seed2
      values:
        - data:
            init_args:
              dataset_args:
                scm_generator_args:
                  seed: 10
        - data:
            init_args:
              dataset_args:
                scm_generator_args:
                  seed: 111
        - data:
            init_args:
              dataset_args:
                scm_generator_args:
                  seed: 120
    sweep_group_limited_basis_toggle:
      sweep_identifier: toggle_limit_basis
      sweep: True
      sweep_alias:
        - limited
        - unlimited(64)
      values:
        - model:
            init_args:
              model_args:
                permutation_learner_args:
                  # Don't change for that particular sweep and let it be limited basis
                  maximum_basis_size: sweep_as_is
        - model:
            init_args:
              model_args:
                permutation_learner_args:
                  maximum_basis_size: 64
