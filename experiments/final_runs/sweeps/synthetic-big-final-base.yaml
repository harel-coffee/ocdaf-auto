# This is a sweep on different phase changing strategies and how
# it can affect the training process for larger datasets
trainer:
  # callbacks
  callbacks:
    - class_path: ocd.training.callbacks.data_visualizer.DataVisualizer
    - class_path: ocd.training.callbacks.phase_changer.PhaseChangerCallback
      init_args:
        maximization_epoch_limit: 100
        expectation_epoch_limit: 80
        patience: 15
        cooldown: 200
        monitor_validation: False
        monitor_training: True
        reset_optimizers: False
        reinitialize_weights_on_maximization: False
    - class_path: ocd.training.callbacks.save_results.SavePermutationResultsCallback
      init_args:
        save_path: experiments/final_runs/sweeps/synthetic-small-parametric
        num_samples: 5000
        log_every_n_epochs: 5
  accelerator: gpu
  devices: 1
  num_nodes: 0
  log_every_n_steps: 1
  check_val_every_n_epoch: 1
  enable_checkpointing: true
  enable_model_summary: true
  enable_progress_bar: false
  # max_epochs: 3200
  max_epochs: 1
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: smart-trainer
data:
  class_path: lightning_toolbox.DataModule
  init_args:
    batch_size: 128
    dataset: ocd.data.SyntheticOCDDataset
    dataset_args:
      standardization: True
      reject_outliers_n_far_from_mean: 5
      name: synthetic_big
      observation_size: 1000
    val_size: 0.01

model:
  class_path: ocd.training.module.OCDafTrainingModule
  init_args:
    # the model
    model_cls: ocd.models.ocdaf.OCDAF
    model_args:
      # Permutation learner arguments
      use_permutation: True
      permutation_learner_cls: ocd.models.permutation.LearnablePermutation
      permutation_learner_args:
        gumbel_noise_std:
          code: |
            def func(self, training_module, **kwargs):
              tot_epochs_on_exp = 3200 / 180 * 80
              step_size = 2.0 / tot_epochs_on_exp
              if not hasattr(training_module, 'last_std'):
                training_module.last_std = 2.0
              
              # if you don't have the epoch from before then assign it
              if not hasattr(training_module, 'last_epoch_std_sched'):
                training_module.last_epoch_std_sched = training_module.current_epoch
              
              if training_module.current_phase != 'maximization' and training_module.last_epoch_std_sched != training_module.current_epoch:
                training_module.last_std -= step_size
              
              training_module.last_epoch_std_sched = training_module.current_epoch

              return training_module.last_std
          function_of_interest: func
        permutation_type: hybrid-sparse-map-simulator
      # Base distribution arguments
      base_distribution: torch.distributions.Normal
      base_distribution_args:
        loc: 0.0
        scale: 1.0
      # The flow model arguments
      layers: [10, 30, 10]
      populate_features: True
      layers_limit: [100, 300, 50]
      num_transforms: 1
      additive: False
      residual: False
      bias: true
      # activations
      activation: torch.nn.LeakyReLU
      activation_args:
        negative_slope: 0.1
    # the optimizer
    optimizer_parameters:
      - model.flow
      - model.permutation_model
    optimizer_is_active:
      - >
        lambda training_module: training_module.current_phase == 'maximization' if hasattr(training_module, 'current_phase') else True
      - >
        lambda training_module: training_module.current_phase == 'expectation' if hasattr(training_module, 'current_phase') else True
    grad_clip_val: 1.0
    lr: [0.01, 0.01]
    optimizer: [torch.optim.AdamW, torch.optim.AdamW]
    optimizer_args:
      - weight_decay: 0.1
      - weight_decay: 0.01
    scheduler:
      - torch.optim.lr_scheduler.ReduceLROnPlateau
      - torch.optim.lr_scheduler.ReduceLROnPlateau
    scheduler_args:
      - mode: min
        min_lr: 0.00005
        threshold: 0.001
        factor: 0.5
        patience: 140
      - mode: min
        min_lr: 0.0005
        threshold: 0.001
        factor: 0.5
        patience: 110
    scheduler_name: ["lr_scheduler_maximization", "lr_scheduler_expectation"]
    scheduler_optimizer: [0, 1]
    scheduler_monitor: ["loss", "loss"]
    objective_args:
      nll:
        code: >
          def func(training_module, batch):
            t = training_module.forward(batch)
            res = t['log_prob']
            return -res.mean()
        function_of_interest: func
