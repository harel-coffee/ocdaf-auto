# Only compatible with the smart_trainer
# set seed_everything=111
trainer:
  accelerator: gpu
  strategy: auto
  devices: 1
  num_nodes: 0
  precision: 32-true
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      name: null
      save_dir: .
      version: null
      offline: false
      dir: null
      id: null
      anonymous: null
      project: Sachs
      log_model: false
      experiment: null
      prefix: ""
      checkpoint_name: null
  callbacks:
    - class_path: ocd.training.callbacks.data_visualizer.DataVisualizer
    - class_path: ocd.training.callbacks.phase_changer.PhaseChangerCallback
      init_args:
        starting_phase: maximization
        monitor_validation: false
        monitor_training: true
        maximization_epoch_limit: 100
        expectation_epoch_limit: 50
        patience: 15
        threshold: 0.0001
        cooldown: 100
        reset_optimizers: false
        reinitialize_weights_on_maximization: false
        log_onto_logger: true
    - class_path: ocd.training.callbacks.save_results.SavePermutationResultsCallback
      init_args:
        save_path: experiments/sweep/phasechanging-save-path/smart-non_parametric_non_linear_gaussian_10_2048_chain-2023-05-15-14-33-56
        save_every_n_epochs: null
        log_every_n_epochs: 5
        num_samples: 5000
        ordering_to_score_mapping: null
        log_into_logger: true
        evaluation_metrics: null
        ignore_evaluation_metrics:
          - pc-shd
  max_epochs: 3000
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: null
  check_val_every_n_epoch: 1
  num_sanity_val_steps: null
  log_every_n_steps: 1
  enable_checkpointing: true
  enable_progress_bar: false
  enable_model_summary: true
  accumulate_grad_batches: 1
  gradient_clip_val: null
  gradient_clip_algorithm: null
  deterministic: null
  benchmark: null
  inference_mode: true
  use_distributed_sampler: true
  profiler: null
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: null

data:
  class_path: lightning_toolbox.DataModule
  init_args:
    batch_size: 128
    dataset: ocd.data.real_world.sachs.SachsOCDDataset
    dataset_args:
      name: sachs_standardized
      standardization: True
      reject_outliers_n_far_from_mean: 5
    val_size: 0.01
model:
  class_path: ocd.training.module.OCDafTrainingModule
  init_args:
    maximization_specifics: null
    expectation_specifics: null
    grad_clip_val: 1.0
    phases:
      - maximization
      - expectation
    model: null
    model_cls: ocd.models.ocdaf.OCDAF
    model_args:
      use_permutation: true
      permutation_learner_cls: ocd.models.permutation.LearnablePermutation
      permutation_learner_args:
        permutation_type: hybrid-sparse-map-simulator
        # change this if cuda out of limit happens
        maximum_basis_size: null
        gumbel_noise_std:
          code:
            "def func(self, training_module, **kwargs):\n  tot_epochs_on_exp =\
            \ 3000 / 150 * 50\n  step_size = 2.0 / tot_epochs_on_exp\n  if not hasattr(training_module,\
            \ 'last_std'):\n    training_module.last_std = 2.0\n  \n  # if you don't\
            \ have the epoch from before then assign it\n  if not hasattr(training_module,\
            \ 'last_epoch_std_sched'):\n    training_module.last_epoch_std_sched =\
            \ training_module.current_epoch\n  \n  if training_module.current_phase\
            \ != 'maximization' and training_module.last_epoch_std_sched != training_module.current_epoch:\n\
            \    training_module.last_std -= step_size\n  \n  training_module.last_epoch_std_sched\
            \ = training_module.current_epoch\n\n  return training_module.last_std\n"
          function_of_interest: func
      base_distribution: torch.distributions.Normal
      base_distribution_args:
        loc: 0.0
        scale: 1.0
      layers:
        - 10
        - 30
        - 10
      populate_features: true
      layers_limit:
        - 100
        - 300
        - 50
      num_transforms: 1
      additive: false
      residual: false
      bias: true
      scale_transform: true
      scale_transform_s_args:
        pre_act_scale: 0.1
        post_act_scale: 10.0
      scale_transform_t_args:
        pre_act_scale: 0.01
        post_act_scale: 100.0
      activation: torch.nn.LeakyReLU
      activation_args:
        negative_slope: 0.1
      in_features: 10
    objective: null
    objective_cls: lightning_toolbox.Objective
    objective_args:
      nll:
        code:
          "def func(training_module, batch):\n  t = training_module.forward(batch)\n\
          \  res = t['log_prob']\n  return -res.mean()\n"
        function_of_interest: func
    optimizer:
      - torch.optim.AdamW
      - torch.optim.AdamW
    optimizer_frequency: null
    optimizer_is_active:
      - "lambda training_module: training_module.current_phase == 'maximization' if
        hasattr(training_module, 'current_phase') else True

        "
      - "lambda training_module: training_module.current_phase == 'expectation' if
        hasattr(training_module, 'current_phase') else True

        "
    optimizer_parameters:
      - model.flow
      - model.permutation_model
    optimizer_args:
      - weight_decay: 0.1
      - weight_decay: 0.01
    lr:
      - 0.01
      - 0.01
    scheduler:
      - torch.optim.lr_scheduler.ReduceLROnPlateau
      - torch.optim.lr_scheduler.ReduceLROnPlateau
    scheduler_name:
      - lr_scheduler_maximization
      - lr_scheduler_expectation
    scheduler_optimizer:
      - 0
      - 1
    scheduler_args:
      - mode: min
        factor: 0.8
        patience: 60
        min_lr: 5.0e-06
        threshold: 0.0001
      - mode: min
        factor: 0.8
        patience: 60
        min_lr: 5.0e-06
        threshold: 0.0001
    scheduler_interval: epoch
    scheduler_frequency: 1
    scheduler_monitor:
      - loss
      - loss
    scheduler_strict: null
    save_hparams: true
    initialize_superclass: true
