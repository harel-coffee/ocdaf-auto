seed_everything: 111
trainer:
  logger: true
  enable_checkpointing: true
  callbacks:
  - class_path: ocd.training.callbacks.phase_changer.PhaseChangerCallback
    init_args:
      starting_phase: maximization
      maximization_epoch_limit: 120
      expectation_epoch_limit: 120
      check_every_n_iterations: 1
      patience: 25
      threshold: 0.0001
      cooldown: 50
      reset_optimizers: true
      reinitialize_weights_on_maximization: false
      log_onto_logger: true
  - class_path: ocd.training.callbacks.birkhoff_visualizer.BirkhoffCallback
    init_args:
      evaluate_every_n_epochs: 5
      evaluate_every_n_epoch_logic: null
      epoch_buffer_size: 1
      log_training: true
      log_validation: false
      permutation_size: 3
      seed: 666
      fit_every_time: false
      write_cost_values: true
      write_permutation_names: true
      loss_cluster_count: 100
      core_points_has_birkhoff_vertices: true
      core_points_has_birkhoff_edges: false
      ordering_to_score_mapping: null
      add_permutation_to_name: true
      reject_outlier_factor: 0.1
      causal_graph:
        class_path: networkx.DiGraph
        init_args:
          incoming_graph_data:
          - - 1
            - 2
          - - 0
            - 2
  - class_path: ocd.training.callbacks.save_results.SavePermutationResultsCallback
    init_args:
      save_path: experiments/smart-trainer-logs/parametric_non_linear_gaussian_3_1000_v_structure_sin_plus_x-11
      save_every_n_epochs: null
      log_every_n_epochs: 5
      num_samples: 1000
      ordering_to_score_mapping: null
      causal_graph:
        class_path: networkx.DiGraph
        init_args:
          incoming_graph_data:
          - - 1
            - 2
          - - 0
            - 2
      log_into_logger: true
  - class_path: lightning.pytorch.callbacks.ModelCheckpoint
    init_args:
      dirpath: /ssd003/home/hamidk/ocd/experiments/sweep/checkpoints/j3prpr9d
      filename: null
      monitor: null
      verbose: true
      save_last: true
      save_top_k: -1
      save_weights_only: false
      mode: min
      auto_insert_metric_name: true
      every_n_train_steps: null
      train_time_interval: null
      every_n_epochs: null
      save_on_train_epoch_end: null
  default_root_dir: null
  gradient_clip_val: null
  gradient_clip_algorithm: null
  num_nodes: 0
  num_processes: null
  devices: 1
  gpus: null
  auto_select_gpus: null
  tpu_cores: null
  ipus: null
  enable_progress_bar: false
  overfit_batches: 0.0
  track_grad_norm: -1
  check_val_every_n_epoch: 1
  fast_dev_run: false
  accumulate_grad_batches: null
  max_epochs: 900
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  val_check_interval: null
  log_every_n_steps: 1
  accelerator: gpu
  strategy: null
  sync_batchnorm: false
  precision: 32
  enable_model_summary: true
  num_sanity_val_steps: 2
  resume_from_checkpoint: null
  profiler: null
  benchmark: null
  deterministic: null
  reload_dataloaders_every_n_epochs: 0
  auto_lr_find: false
  replace_sampler_ddp: true
  detect_anomaly: false
  auto_scale_batch_size: false
  plugins: null
  amp_backend: null
  amp_level: null
  move_metrics_to_cpu: false
  multiple_trainloader_mode: max_size_cycle
  inference_mode: true
data:
  class_path: lightning_toolbox.DataModule
  init_args:
    dataset: ocd.data.SyntheticOCDDataset
    dataset_args:
      name: parametric_non_linear_gaussian_3_1000_v_structure_sin_plus_x
      observation_size: 1000
      scm_generator: ocd.data.synthetic.InvertibleModulatedGaussianSCMGenerator
      scm_generator_args:
        graph_generator: ocd.data.scm.GraphGenerator
        graph_generator_args:
          graph_type: v_structure
          n: 3
          seed: 575
        mean: 0.0
        s_function:
          function_descriptor: "def func(x):\n    x[x < 100] = numpy.log(1 + numpy.exp(x[x\
            \ < 100]))\n    return x"
          function_of_interest: func
        s_function_signature: softplus
        seed: 145
        std: 1.0
        t_function:
          function_descriptor: "def func(x):\n    return numpy.sin(x) + x"
          function_of_interest: func
        t_function_signature: sin_plus_x
        weight_s:
        - 0.5
        - 1.5
        weight_t:
        - 0.5
        - 1.5
      seed: 237
    train_dataset: null
    train_dataset_args: null
    val_size: 0.1
    val_dataset: null
    val_dataset_args: null
    test_dataset: null
    test_dataset_args: null
    batch_size: 128
    train_batch_size: null
    val_batch_size: null
    test_batch_size: null
    pin_memory: true
    train_pin_memory: null
    val_pin_memory: null
    test_pin_memory: null
    train_shuffle: true
    val_shuffle: false
    test_shuffle: false
    num_workers: 0
    train_num_workers: null
    val_num_workers: null
    test_num_workers: null
    seed: 0
    save_hparams: true
    initialize_superclass: true
model:
  class_path: ocd.training.module.OCDafTrainingModule
  init_args:
    maximization_specifics: null
    expectation_specifics: null
    grad_clip_val: 1.0
    phases:
    - maximization
    - expectation
    model: null
    model_cls: ocd.models.ocdaf.OCDAF
    model_args:
      use_permutation: true
      permutation_learner_cls: ocd.models.permutation.LearnablePermutation
      permutation_learner_args:
        gumbel_noise_std: 'lambda self, training_module, **kwargs: 2 * ((1 - (1.0
          * training_module.trainer.current_epoch / training_module.trainer.max_epochs))
          ** 3)

          '
        permutation_type: hybrid-sparse-map-simulator
      base_distribution: torch.distributions.normal.Normal
      base_distribution_args:
        loc: 0.0
        scale: 1.0
      in_features: 3
      layers:
      - 30
      - 33
      - 90
      num_transforms: 1
      additive: false
      residual: false
      bias: true
      activation: torch.nn.LeakyReLU
      activation_args:
        negative_slope: 0.1
      clamp_val: 10000000
    objective: null
    objective_cls: lightning_toolbox.Objective
    objective_args:
      nll:
        code: "def func(training_module, batch):\n  t = training_module.forward(batch)\n\
          \  res = t['log_prob']\n  return -res.mean()\n"
        function_of_interest: func
    optimizer:
    - torch.optim.AdamW
    - torch.optim.AdamW
    optimizer_frequency: null
    optimizer_is_active:
    - 'lambda training_module: training_module.current_phase == ''maximization''

      '
    - 'lambda training_module: training_module.current_phase == ''expectation''

      '
    optimizer_parameters:
    - model.flow
    - model.permutation_model
    optimizer_args:
      weight_decay: 100
    lr:
    - 0.01
    - 0.01
    scheduler:
    - ocd.training.schedulers.reduce_on_increase.ReduceLROnIncrease
    - ocd.training.schedulers.reduce_on_increase.ReduceLROnIncrease
    scheduler_name:
    - lr_scheduler_maximization
    - lr_scheduler_expectation
    scheduler_optimizer:
    - 0
    - 1
    scheduler_args:
    - mode: min
      factor: 0.5
      patience: 10
      min_lr: 5.0e-06
      threshold: 0.0001
    - mode: min
      factor: 0.5
      patience: 10
      min_lr: 5.0e-06
      threshold: 0.0001
    scheduler_interval: epoch
    scheduler_frequency: 1
    scheduler_monitor:
    - loss
    - loss
    scheduler_strict: null
    save_hparams: true
    initialize_superclass: true
