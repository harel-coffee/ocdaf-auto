data:
  class_path: lightning_toolbox.DataModule
  init_args:
    batch_size: 128
    dataset: ocd.data.SyntheticOCDDataset
    dataset_args:
      standardization: True
      reject_outliers_n_far_from_mean: 5
      name: parametric_normal_modulated_4_5000_full_sin_plus_x
      observation_size: 1000
      scm_generator: ocd.data.synthetic.ParametricSCMGenerator
      scm_generator_args:
        graph_generator: ocd.data.scm.GraphGenerator
        graph_generator_args: { graph_type: full, n: 4, seed: 867 }
        noise_parameters: { loc: 0.0, scale: 1.0 }
        noise_type: normal
        s_function:
          { function_descriptor: "def func(x):\n    x[x < 100] = numpy.log(1\
              \ + numpy.exp(x[x < 100]))\n    return x", function_of_interest: func }
        s_function_signature: softplus
        seed: 573
        t_function:
          { function_descriptor: "def func(x):\n    return numpy.sin(x)\
              \ + x", function_of_interest: func }
        t_function_signature: sin_plus_x
        weight_s: [0.5, 1.5]
        weight_t: [0.5, 1.5]
      seed: 828
    val_size: 0.01
model:
  class_path: ocd.training.module.OCDafTrainingModule
  init_args:
    # the model
    model_cls: ocd.models.ocdaf.OCDAF
    model_args:
      # Permutation learner arguments
      use_permutation: True
      permutation_learner_cls: ocd.models.permutation.LearnablePermutation
      permutation_learner_args:
        sinkhorn_temp: >
          lambda self, training_module, **kwargs: 0.2
        gumbel_noise_std: >
          lambda self, training_module, **kwargs: 2 - (2 / (training_module.trainer.max_epochs)) * (training_module.current_epoch)
        permutation_type: hybrid-sparse-map-simulator
      base_distribution: torch.distributions.Normal
      base_distribution_args:
        loc: 0.0
        scale: 1.0
      # The flow model arguments
      layers: [11, 33, 11]
      populate_features: True
      num_transforms: 1
      additive: False
      residual: False
      bias: true
      activation: torch.nn.LeakyReLU
      activation_args:
        negative_slope: 0.1
      scale_transform: True
      scale_transform_s_args:
        pre_act_scale: 0.4
        post_act_scale: 2.5
      scale_transform_t_args:
        pre_act_scale: 0.1
        post_act_scale: 10.0

    # the optimizer
    optimizer: [torch.optim.AdamW, torch.optim.AdamW]
    optimizer_args:
      - weight_decay: 0.25
      - weight_decay: 0.01
    optimizer_parameters:
      - model.flow
      - model.permutation_model
    optimizer_is_active:
      - >
        lambda training_module: training_module.current_phase == 'maximization' if hasattr(training_module, 'current_phase') else True
      - >
        lambda training_module: training_module.current_phase == 'expectation' if hasattr(training_module, 'current_phase') else True
    grad_clip_val: 1.0
    lr: [0.005, 0.005]
    scheduler:
      - ocd.training.schedulers.reduce_on_increase.ReduceLROnIncrease
      - ocd.training.schedulers.reduce_on_increase.ReduceLROnIncrease
    scheduler_args:
      - mode: min
        factor: 0.8
        patience: 100
        min_lr: 0.000005
        threshold: 0.0001
      - mode: min
        factor: 0.8
        patience: 100
        min_lr: 0.000005
        threshold: 0.0001
    scheduler_name: ["lr_scheduler_maximization", "lr_scheduler_expectation"]
    scheduler_optimizer: [0, 1]
    # # # # scheduler_interval: ["epoch", "epoch"]
    # # # # # scheduler_frequency: [22500, 22500]
    scheduler_monitor: ["loss", "loss"]
    # the loss
    # objective_cls: ocd.training.terms.NLLTerm
    objective_args:
      nll:
        code: >
          def func(training_module, batch):
            t = training_module.forward(batch)
            res = t['log_prob']
            return -res.mean()
        function_of_interest: func
trainer:
  # callbacks
  callbacks:
    - class_path: ocd.training.callbacks.phase_changer.PhaseChangerCallback
      init_args:
        starting_phase: maximization
        # The settings regarding epoch limit values
        maximization_epoch_limit: 80
        expectation_epoch_limit: 50
        # The settings regarding the generalization gap
        patience: 25
        threshold: 0.0001
        cooldown: 50
        reset_optimizers: False
        reinitialize_weights_on_maximization: False
    - class_path: ocd.training.callbacks.birkhoff_visualizer.BirkhoffCallback
      init_args:
        epoch_buffer_size: 1
        evaluate_every_n_epochs: 5
        seed: 666
        write_cost_values: True
        write_permutation_names: True
        core_points_has_birkhoff_vertices: True
        core_points_has_birkhoff_edges: False
        reject_outlier_factor: 0.1
        # Include permutation names
        add_permutation_to_name: True
    - class_path: ocd.training.callbacks.save_results.SavePermutationResultsCallback
      init_args:
        save_path: experiments/simple/my-saves
        num_samples: 1000
        log_every_n_epochs: 5
    - class_path: ocd.training.callbacks.checkpointing.DebuggedModelCheckpoint
      init_args:
        dirpath: experiments/smart/checkpoints
        save_last: true
        verbose: true
        every_n_epochs: 1
  accelerator: gpu
  devices: 1
  num_nodes: 0
  max_epochs: 1000 # SWEEP on this
  log_every_n_steps: 1
  check_val_every_n_epoch: 1
  enable_checkpointing: true
  enable_model_summary: true
  enable_progress_bar: false
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: smart-trainer
