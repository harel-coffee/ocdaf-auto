project: my-sweep-project
run_name: random-name
checkpoint_interval: 30
default_root_dir: experiments/sweep
sweep_id: null
agent_run_args:
  count: 10
sweep_configuration:
  method: grid
  metric:
    goal: minimize
    name: metrics/best-backward_relative_penalty
  parameters:
    model:
      class_path: ocd.training.module.OCDafTrainingModule
      init_args:
        model_cls: ocd.models.ocdaf.OCDAF
        model_args:
          use_permutation: True
          permutation_learner_cls: ocd.models.permutation.LearnablePermutation
          permutation_learner_args:
            gumbel_noise_std:
              sweep: True
              values:
              - >
                lambda self, training_module, **kwargs: 2 * ((1 - (1.0 * training_module.trainer.current_epoch / training_module.trainer.max_epochs)) ** 3)
              - >
                lambda self, training_module, **kwargs: 2 - (2.0 / (training_module.trainer.max_epochs)) * (training_module.current_epoch)
              values_display_name:
              - exponential
              - linear
            permutation_type: hybrid-sparse-map-simulator
          # Base distribution arguments
          base_distribution: torch.distributions.Normal
          base_distribution_args:
            loc: 0.0
            scale: 1.0
          layers: 
            sweep: True
            unique_name: custom_layers
            values:
            - [10, 11, 30]
            - [5, 10, 6]
          num_transforms:
            sweep: True
            values:
            - 1
            - 3
          additive: False
          residual: False
          bias: true
          activation: torch.nn.LeakyReLU
          activation_args:
            negative_slope: 0.1
          clamp_val: 10000000
        

        # the optimizer
        optimizer: [torch.optim.AdamW, torch.optim.AdamW]
        optimizer_args:
          weight_decay: 
            sweep: True
            values:
            - 100.0
            - 1
            - 0.5
            - 0.25
            - 0.1
            - 0.01
        optimizer_parameters:
          - model.flow
          - model.permutation_model
        optimizer_is_active: 
          - > 
            lambda training_module: training_module.current_phase == 'maximization'
          - >
            lambda training_module: training_module.current_phase == 'expectation'
        grad_clip_val: 1.0
        lr: [0.01, 0.01]
        scheduler: 
          - ocd.training.schedulers.reduce_on_increase.ReduceLROnIncrease
          - ocd.training.schedulers.reduce_on_increase.ReduceLROnIncrease
        scheduler_args:
          - mode: min
            factor: 0.5
            patience: 10
            min_lr: 0.000005
            threshold: 0.0001
          - mode: min
            factor: 0.5
            patience: 10
            min_lr: 0.000005
            threshold: 0.0001
        scheduler_name: ["lr_scheduler_maximization", "lr_scheduler_expectation"]
        scheduler_optimizer: [0, 1]
        # # # # scheduler_interval: ["epoch", "epoch"]
        # # # # # scheduler_frequency: [22500, 22500]
        scheduler_monitor: ["loss", "loss"]
        # # the loss
        # # objective_cls: ocd.training.terms.NLLTerm 
        objective_args:
          nll:
            code: >
              def func(training_module, batch):
                t = training_module.forward(batch)
                res = t['log_prob']
                return -res.mean()
            function_of_interest: func